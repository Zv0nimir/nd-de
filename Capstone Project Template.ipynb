{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The idea of this project is to build ETL pipeline that aquires the I94 immigration data and city temperature to answer questions regarding average temperature and tourist arrivals to certian cities.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, year, month, dayofmonth, hour, weekofyear, date_format, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('project.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "input_data= config.get('PATH', 'INPUT_DATA')\n",
    "output_data= config.get('PATH', 'OUTPUT_DATA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-c3e06846": {
       "style": "primary"
      }
     }
    }
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, I will create ETL pipeline that aquires and aggregates the I94 immigration data and average temperature date by destination city for April, 2016. I will use provided data sources to build stage tables. Next step will be to create fact table and necessary dimension tables. To create this solution, I will use Pandas, PySpark and AWS S3 buckets.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "* **I94 immigration data** - comes from the US National Tourism and Trade Office ([NTTO](https://www.trade.gov/national-travel-and-tourism-office)), provided by Udacity. The data is provided in [sas7bdat  binary file format](https://cran.r-project.org/web/packages/sas7bdat/vignettes/sas7bdat.pdf). I will use the following attributes:\n",
    "    - i94cit - code of visitor origin country\n",
    "    - i94port - code of destination city in the US\n",
    "    - i94mode - code of transportation mode\n",
    "    - arrdate - arrival date to the US\n",
    "    - depdate - departure date from the US\n",
    "    - i94visa - code of visa type \n",
    "    - visatype - class of admission legally admitting the non-immigrant to temporarily stay in US\n",
    "    \n",
    "    \n",
    "* **City temperature data** - dataset comes from [Kaggle](https://www.kaggle.com/sudalairajkumar/daily-temperature-of-major-cities). The dataset is in [csv](https://docs.fileformat.com/spreadsheet/csv/) format. I will use the following attributes:\n",
    "    - date\n",
    "    - city\n",
    "    - state\n",
    "    - average temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    "                    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get I94 immigration data\n",
    "df_spark_i94 = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark_i94.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- AvgTemperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_temperature = spark.read.options(header='True', inferSchema='True').csv(os.path.join(input_data,'city_temperature.csv'))\n",
    "df_spark_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "\n",
    "Data quality issues:\n",
    "\n",
    "* **I94 immigration data**\n",
    "    - invalid ports, some port codes are missing or does not belong to ports in the US\n",
    "    - missing values for departure date - cca 4.6% of records have no values\n",
    "    - missing values for arrival mode - cca 0.0077% of records have no values\n",
    "    - missing values for match of arrival and departure records - cca 4.47 of records have no values\n",
    "    \n",
    "    \n",
    "* **City temperature data**\n",
    "    - worldwide data - only records for US ports for April 2016 are necessary\n",
    "    - convert the columns City and State to uppercase to speed up joining with city mapping list where the names are in upper case\n",
    "        \n",
    "#### Cleaning Steps\n",
    "\n",
    "* **I94 immigration data**\n",
    "    - remove records with invalid ports\n",
    "    - remove records without departure date\n",
    "    - remove records without arrival mode\n",
    "    - remove records without match of arrival and departure records\n",
    "    \n",
    "    \n",
    "* **City temperature data**\n",
    "    - filter the records to get data for US ports for April 2016\n",
    "    - change column case for columns City and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create dictionary of valid ports\n",
    "port_re_filter = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "i94_port_dict = {}\n",
    "\n",
    "with open(os.path.join(input_data, 'map/I94-us_ports.txt')) as file:\n",
    "    for line in file:\n",
    "        groups = port_re_filter.search(line)\n",
    "        if 'No PORT Code' not in groups[2] and 'Collapsed' not in groups[2]:\n",
    "            i94_port_dict[groups[1]] = groups[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#clean I94 immigration data\n",
    "df_spark_i94_clean = df_spark_i94.dropDuplicates() \\\n",
    "                                .filter(df_spark_i94.i94port.isin(list(i94_port_dict.keys()))) \\\n",
    "                                .na.drop(subset=[\"depdate\"]) \\\n",
    "                                .na.drop(subset=[\"i94mode\"]) \\\n",
    "                                .na.drop(subset=[\"matflag\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_i94_clean.write.mode('overwrite').partitionBy('arrdate').parquet(os.path.join(output_data, \"stage_i94_immigration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_temperature = df_spark_temperature.toDF(*[c.lower() for c in df_spark_temperature.columns])\n",
    "df_spark_temperature_clean = df_spark_temperature.dropDuplicates() \\\n",
    "                                                .filter(df_spark_temperature.country == 'US') \\\n",
    "                                                .filter(df_spark_temperature.year == 2016) \\\n",
    "                                                .filter(df_spark_temperature.month == 4) \\\n",
    "                                                .withColumn('state', upper(df_spark_temperature.state)) \\\n",
    "                                                .withColumn('city', upper(df_spark_temperature.city)) \\\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_temperature_clean.write.mode('overwrite').parquet(os.path.join(output_data, \"stage_city_temperatures\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "As a concept, I will use the following star schema:\n",
    "\n",
    "<img src=\"project_model.png\" alt=\"Conceptual model\" title=\"Conceptual data model\" />\n",
    "\n",
    "* **Fact table**\n",
    "    - fact_i94_visits - contains data from I94 immigration data joined with daily average temperature by the US port city and arrival data\n",
    "    \n",
    "    \n",
    "* **Dimension tables**\n",
    "    - dim_countries - contains list of countries, it gives an information about which country does visitor comes from\n",
    "    - dim_travel_modes - contains list of travel modes, it gives an information about transformation mode used by visitor to come to the US\n",
    "    - dim_us_visas - contains list of visas, it gives an information about the reason of visiting US\n",
    "    - dim_date - contains date information\n",
    "    - dim_us_ports - contains list of US ports, it gives an information about which city or state do visitors visit\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The ETL pipeline steps are described bellow:\n",
    "\n",
    "<ol>\n",
    "    <li>Load raw data to the Spark dataframes: <em>df_spark_i94</em> and <em>df_spark_temperature</em></li>\n",
    "    <li>Clean each Spark dataframe and write each cleaned dataframe into parquet as staging tables: <em>stage_i94_immigration</em> and <em>stage_city_temperatures</em></li>\n",
    "    <li>Create and write dimension tables into parquet: <em>dim_countries</em>, <em>dim_travel_modes</em>, <em>dim_us_visas</em> and <em>dim_us_ports</em></li>\n",
    "    <li>Create and write fact table <em>fact_i94_visits</em> by joining staging tables <em>stage_i94_immigration</em> and <em>stage_city_temperatures</em></li>\n",
    "    <li>Run quality checks</li>\n",
    "</ol>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "Prepare dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create and write dimension table for countries\n",
    "df_countries = pd.read_csv(os.path.join(input_data, 'map/I94-country-codes.txt'), sep=\"=\", header=None, names = ['code', 'name'])\n",
    "df_countries['code'].apply(pd.to_numeric)\n",
    "df_countries['name'] = df_countries['name'].str.replace(\"'\",\"\").replace(to_replace=['No Country.*', 'INVALID.*', 'Collapsed.*'], value = 'Other countries', regex = True).str.strip()\n",
    "\n",
    "df_spark_countries = spark.createDataFrame(df_countries)\n",
    "df_spark_countries.write.mode('overwrite').parquet(os.path.join(output_data, \"dim_countries\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create and write dimension table for travel modes\n",
    "df_travel_modes = pd.read_csv(os.path.join(input_data, 'map/I94-travel_modes.txt'), sep=\"=\", header=None, names = ['code', 'name'])\n",
    "df_travel_modes['code'].apply(pd.to_numeric)\n",
    "df_travel_modes['name'] = df_travel_modes['name'].str.replace(\"'\",\"\").str.strip()\n",
    "\n",
    "\n",
    "df_spark_travel_modes = spark.createDataFrame(df_travel_modes)\n",
    "df_spark_travel_modes.write.mode('overwrite').parquet(os.path.join(output_data, \"dim_travel_modes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create and write dimension table for US visas\n",
    "df_us_visas = pd.read_csv(os.path.join(input_data, 'map/I94-us_visas.txt'), sep=\"=\", header=None, names = ['code', 'name'])\n",
    "df_us_visas['code'].apply(pd.to_numeric)\n",
    "df_us_visas['name'].str.strip()\n",
    "\n",
    "df_spark_us_visas = spark.createDataFrame(df_us_visas)\n",
    "df_spark_us_visas.write.mode('overwrite').parquet(os.path.join(output_data, \"dim_us_visas\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create and write dimension table for US ports\n",
    "df_us_ports = pd.read_csv(os.path.join(input_data, 'map/I94-us_ports.txt'), sep=\"=\", header=None, names = ['code', 'city'])\n",
    "df_us_ports['code'] = df_us_ports['code'].str.replace(\"'\",\"\").str.replace(\"\\t\",\"\").str.strip()\n",
    "df_us_ports['city'] = df_us_ports['city'].str.replace(\"'\",\"\").str.replace(\"\\t\",\"\").replace(to_replace=['No PORT Code.*', 'Collapsed.*'], value = 'Other US ports', regex = True).str.strip()\n",
    "\n",
    "us_ports_split_col = df_us_ports['city'].str.split(\",\", n=1, expand=True)\n",
    "\n",
    "df_us_ports['city'] = us_ports_split_col[0].str.strip()\n",
    "\n",
    "df_us_ports['state'] = us_ports_split_col[1]\n",
    "df_us_ports['state'] = df_us_ports['state'].str.strip()\n",
    "\n",
    "df_us_states = pd.read_csv(os.path.join(input_data, 'map/I94-us_states.txt'), sep=\"=\", header=None, names = ['state_code', 'state_name'])\n",
    "df_us_states['state_code'] = df_us_states['state_code'].str.replace(\"'\",\"\").str.replace(\"\\t\",\"\").str.strip()\n",
    "df_us_states['state_name'] = df_us_states['state_name'].str.replace(\"'\",\"\").str.replace(\"\\t\",\"\").str.strip()\n",
    "\n",
    "df_us_ports = df_us_ports.join(df_us_states.set_index('state_code'), on='state', how='inner', lsuffix='p', rsuffix='s')\n",
    "\n",
    "df_spark_us_ports = spark.createDataFrame(df_us_ports)\n",
    "df_spark_us_ports.write.mode('overwrite').parquet(os.path.join(output_data, \"dim_us_ports\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Build fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+-------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|arrdate|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+-------+\n",
      "|5659207.0|2016.0|   4.0| 103.0| 103.0|    NEW|    1.0|     NJ|20582.0|  55.0|    2.0|  1.0|20160430|    null| null|      G|      O|   null|      M| 1961.0|07282016|     F|  null|     OS|5.9529151033E10|00089|      WT|20574.0|\n",
      "|5659318.0|2016.0|   4.0| 103.0| 103.0|    HOU|    1.0|     UT|20595.0|  51.0|    2.0|  1.0|20160430|    null| null|      O|      O|   null|      M| 1965.0|07282016|  null|  null|     UA|5.9537002333E10|00160|      WT|20574.0|\n",
      "|5659356.0|2016.0|   4.0| 103.0| 103.0|    NYC|    1.0|     NY|20578.0|  29.0|    2.0|  1.0|20160430|    null| null|      G|      O|   null|      M| 1987.0|07282016|     F|  null|     OS|5.9545262433E10|00087|      WT|20574.0|\n",
      "|5659405.0|2016.0|   4.0| 103.0| 103.0|    NYC|    1.0|     NY|20581.0|  55.0|    2.0|  1.0|20160430|    null| null|      G|      O|   null|      M| 1961.0|07282016|     F|  null|     OS|5.9544306333E10|00087|      WT|20574.0|\n",
      "|5659433.0|2016.0|   4.0| 103.0| 103.0|    NYC|    1.0|     NY|20582.0|  46.0|    2.0|  1.0|20160430|    null| null|      G|      O|   null|      M| 1970.0|07282016|     M|  null|     OS|5.9543230133E10|00087|      WT|20574.0|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read immigration data from parquet\n",
    "df_fact_I94_visits = spark.read.parquet(os.path.join(output_data, 'stage_i94_immigration'))\n",
    "df_fact_I94_visits.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----------+--------------+-----+---+----+--------------+\n",
      "|       region|country|      state|          city|month|day|year|avgtemperature|\n",
      "+-------------+-------+-----------+--------------+-----+---+----+--------------+\n",
      "|North America|     US|   ARKANSAS|   LITTLE ROCK|    4|  3|2016|          59.8|\n",
      "|North America|     US| CALIFORNIA|     SAN DIEGO|    4| 26|2016|          61.1|\n",
      "|North America|     US|   COLORADO|GRAND JUNCTION|    4| 13|2016|          56.7|\n",
      "|North America|     US|CONNECTICUT|    BRIDGEPORT|    4| 10|2016|          41.0|\n",
      "|North America|     US|    FLORIDA|  JACKSONVILLE|    4| 29|2016|          75.6|\n",
      "+-------------+-------+-----------+--------------+-----+---+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read temperature data from parquet\n",
    "df_dim_temperatures = spark.read.parquet(os.path.join(output_data, \"stage_city_temperatures\"))\n",
    "df_dim_temperatures.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+----------+\n",
      "|code|                city|state|state_name|\n",
      "+----+--------------------+-----+----------+\n",
      "| ALC|               ALCAN|   AK|    ALASKA|\n",
      "| ANC|           ANCHORAGE|   AK|    ALASKA|\n",
      "| BAR|BAKER AAF - BAKER...|   AK|    ALASKA|\n",
      "| DAC|       DALTONS CACHE|   AK|    ALASKA|\n",
      "| PIZ|DEW STATION PT LA...|   AK|    ALASKA|\n",
      "+----+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read US ports data from parquet\n",
    "df_dim_us_ports = spark.read.parquet(os.path.join(output_data, \"dim_us_ports\"))\n",
    "df_dim_us_ports.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create udf to convert SAS date type to ISO date type\n",
    "get_date_from_sas = udf(lambda x: (datetime(1960, 1, 1).date() + timedelta(x)).isoformat() if x else None)\n",
    "\n",
    "#create udf to get date from date parts\n",
    "get_date = udf(lambda x, y, z: (datetime(x, y, z).date()).isoformat())\n",
    "\n",
    "#create udf to get visitor's stay\n",
    "get_stay = udf(lambda x, y: int(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+----------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+----------+----+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|i94mode|i94addr|   depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|   arrdate|stay|\n",
      "+---------+------+------+------+------+-------+-------+-------+----------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+----------+----+\n",
      "|5659207.0|2016.0|   4.0| 103.0| 103.0|    NEW|    1.0|     NJ|2016-05-08|  55.0|    2.0|  1.0|20160430|    null| null|      G|      O|   null|      M| 1961.0|07282016|     F|  null|     OS|5.9529151033E10|00089|      WT|2016-04-30|   8|\n",
      "|5659318.0|2016.0|   4.0| 103.0| 103.0|    HOU|    1.0|     UT|2016-05-21|  51.0|    2.0|  1.0|20160430|    null| null|      O|      O|   null|      M| 1965.0|07282016|  null|  null|     UA|5.9537002333E10|00160|      WT|2016-04-30|  21|\n",
      "|5659356.0|2016.0|   4.0| 103.0| 103.0|    NYC|    1.0|     NY|2016-05-04|  29.0|    2.0|  1.0|20160430|    null| null|      G|      O|   null|      M| 1987.0|07282016|     F|  null|     OS|5.9545262433E10|00087|      WT|2016-04-30|   4|\n",
      "|5659405.0|2016.0|   4.0| 103.0| 103.0|    NYC|    1.0|     NY|2016-05-07|  55.0|    2.0|  1.0|20160430|    null| null|      G|      O|   null|      M| 1961.0|07282016|     F|  null|     OS|5.9544306333E10|00087|      WT|2016-04-30|   7|\n",
      "|5659433.0|2016.0|   4.0| 103.0| 103.0|    NYC|    1.0|     NY|2016-05-08|  46.0|    2.0|  1.0|20160430|    null| null|      G|      O|   null|      M| 1970.0|07282016|     M|  null|     OS|5.9543230133E10|00087|      WT|2016-04-30|   8|\n",
      "+---------+------+------+------+------+-------+-------+-------+----------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prepare df_fact_I94_visits table\n",
    "df_fact_I94_visits = df_fact_I94_visits.withColumn('stay', get_stay(df_fact_I94_visits.depdate, df_fact_I94_visits.arrdate)) \\\n",
    "                                        .withColumn('arrdate', get_date_from_sas(df_fact_I94_visits.arrdate)) \\\n",
    "                                        .withColumn('depdate', get_date_from_sas(df_fact_I94_visits.depdate)) \\\n",
    "                                        \n",
    "df_fact_I94_visits.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----------+--------------+-----+---+----+--------------+----------+\n",
      "|       region|country|      state|          city|month|day|year|avgtemperature|      date|\n",
      "+-------------+-------+-----------+--------------+-----+---+----+--------------+----------+\n",
      "|North America|     US|   ARKANSAS|   LITTLE ROCK|    4|  3|2016|          59.8|2016-04-03|\n",
      "|North America|     US| CALIFORNIA|     SAN DIEGO|    4| 26|2016|          61.1|2016-04-26|\n",
      "|North America|     US|   COLORADO|GRAND JUNCTION|    4| 13|2016|          56.7|2016-04-13|\n",
      "|North America|     US|CONNECTICUT|    BRIDGEPORT|    4| 10|2016|          41.0|2016-04-10|\n",
      "|North America|     US|    FLORIDA|  JACKSONVILLE|    4| 29|2016|          75.6|2016-04-29|\n",
      "+-------------+-------+-----------+--------------+-----+---+----+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prepare df_dim_temperatures temperature table\n",
    "df_dim_temperatures = df_dim_temperatures.withColumn('date', get_date(df_dim_temperatures.year, df_dim_temperatures.month, df_dim_temperatures.day))\n",
    "df_dim_temperatures.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#join df_fact_I94_visits dataframe with df_dim_us_ports dataframe\n",
    "df_fact_I94_visits = df_fact_I94_visits.join(df_dim_us_ports, df_fact_I94_visits.i94port == df_dim_us_ports.code, how='inner').drop(df_dim_us_ports.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#join df_fact_I94_visits dataframe with df_dim_temperatures dataframe\n",
    "df_fact_I94_visits = df_fact_I94_visits.join(df_dim_temperatures, [df_fact_I94_visits.arrdate == df_dim_temperatures.date, df_fact_I94_visits.city == df_dim_temperatures.city, df_fact_I94_visits.state_name == df_dim_temperatures.state] , how='inner').drop(df_dim_temperatures.city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#select necessary columns and write fact table partitioned by arrival date\n",
    "df_fact_I94_visits = df_fact_I94_visits.select('cicid', 'arrdate', 'depdate', 'stay', 'i94port', 'i94cit', 'i94mode', 'i94visa', 'visatype', 'avgtemperature')\n",
    "\n",
    "df_fact_I94_visits.write.mode('overwrite').partitionBy('arrdate').parquet(os.path.join(output_data, \"fact_i94_visits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create and write dimension table for date\n",
    "df_date = df_fact_I94_visits.select('arrdate').dropDuplicates()\n",
    "\n",
    "df_date = df_date.withColumn('day', dayofmonth(df_date.arrdate)) \\\n",
    "                 .withColumn('weekday', date_format(df_date.arrdate, 'E')) \\\n",
    "                 .withColumn('week', weekofyear(df_date.arrdate)) \\\n",
    "                 .withColumn('month', month(df_date.arrdate)) \\\n",
    "                 .withColumn('year', year(df_date.arrdate))\n",
    "\n",
    "df_date.write.mode('overwrite').parquet(os.path.join(output_data, \"dim_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "To ensure the pipeline ran as expected, I need to perform a number of quality checks. I have prepared the following:\n",
    " * Integrity constraints on the relational database to check if:\n",
    "     * certain column exists on certain table\n",
    "     * certain column with certain data type exists on certain table\n",
    "     * logical primary key is really primary key (unique key check) \n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    "     * check if there is any arrivals outside expected period (non-April 2016)\n",
    " * Source/Count checks to ensure completeness\n",
    "     * check if there are rows in certain tables\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "def check_column_exists(path, table, column):\n",
    "    df = spark.read.parquet(os.path.join(path, table)).limit(1)\n",
    "    if column in df.columns:\n",
    "        print('Column \"{}\" DOES exists in table \"{}\"'.format(column, table))\n",
    "    else:\n",
    "        raise ValueError('Column \"{}\" DOES NOT exists in table \"{}\"'.format(column, table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column \"cicid\" DOES exists in table \"fact_i94_visits\"\n"
     ]
    }
   ],
   "source": [
    "check_column_exists(output_data, \"fact_i94_visits\", \"cicid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_column_type(path, table, column, expected_type):\n",
    "    df = spark.read.parquet(os.path.join(path, table)).limit(1)\n",
    "    \n",
    "    column_exists = False\n",
    "    for name, dtype in df.dtypes:\n",
    "        if name == column:\n",
    "            column_exists = True\n",
    "            if dtype == expected_type:\n",
    "                print('Column \"{}.{}\" HAS expected data type'.format(table, column)) \n",
    "                return\n",
    "            else:\n",
    "                raise ValueError('Column \"{}.{}\" DOES NOT HAVE expected data type'.format(column, table))\n",
    "\n",
    "    if not column_exists: \n",
    "        raise ValueError('Column \"{}\" DOES NOT exists in table \"{}\"'.format(column, table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column \"fact_i94_visits.cicid\" HAS expected data type\n"
     ]
    }
   ],
   "source": [
    "check_column_type(output_data, \"fact_i94_visits\", \"cicid\", \"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_unit_test_on_I94_arrival_date(path):\n",
    "    df = spark.read.parquet(os.path.join(path, \"fact_i94_visits\"))\n",
    "    \n",
    "    rec_count = df.filter((year(df.arrdate) != 2016) | (month(df.arrdate) != 4)).count()\n",
    "                          \n",
    "    if rec_count == 0:\n",
    "        print(\"Unit test has passed. Fact table contains only records for April 2016.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unit test has failed. Fact table contains records outside expected period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit test has passed. Fact table contains only records for April 2016.\n"
     ]
    }
   ],
   "source": [
    "#run unit test to check arrival dates in fact table\n",
    "run_unit_test_on_I94_arrival_date(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_row_count(path, table):\n",
    "    df = spark.read.parquet(os.path.join(path, table))\n",
    "    \n",
    "    rec_count = df.count()\n",
    "    \n",
    "    if rec_count == 0:\n",
    "        raise ValueError('Quality check for table \"{}\" has failed. The table has no records.'.format(table))\n",
    "    else:\n",
    "        print(\"Succesfull quality check on table {}. The table has {} records.\".format(table, rec_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfull quality check on table dim_us_ports. The table has 498 records.\n"
     ]
    }
   ],
   "source": [
    "#run quality check by checking the row count\n",
    "check_row_count(output_data, \"dim_us_ports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_unique_key(path, table, column_list):\n",
    "    df = spark.read.parquet(os.path.join(path, table))\n",
    "    \n",
    "    rec_count = df.count()\n",
    "    \n",
    "    if df.count() > df.dropDuplicates(column_list).count():\n",
    "        raise ValueError('Error checking unique key on table. Key has duplicates for {}'.format(table, column_list))\n",
    "    else:\n",
    "        print(\"Succesfull unique key check on table {}.\".format(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfull unique key check on table fact_i94_visits.\n"
     ]
    }
   ],
   "source": [
    "#run quality check by checking the unique key\n",
    "check_unique_key(output_data, \"fact_i94_visits\", ['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#end spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "Fact table\n",
    "\n",
    "***fact_i94_visits*** - extracted from I94 immigration data for April, 2016; combined with city temperature data provided by Kaggle.\n",
    " * cicid - double - primary key\n",
    " * arrdate - date - date of arrival\n",
    " * depdate - date - date of departure\n",
    " * stay - int - number of days that visitor has stayed in the US\n",
    " * i94port - varchar - three-character code of destination port/city\n",
    " * i94cit - int - three-digit code of visitor's origin country\n",
    " * i94mode - int - one-digit code of transportation mode\n",
    " * i94visa - int - one-digit code of visa type\n",
    " * visatype - varchar - class of admission legally admitting the non-immigrant to temporarily stay in US\n",
    " \n",
    "Dimension tables\n",
    "\n",
    "***dim_countries*** - extracted from mapping text, which is provided by Udacity\n",
    " * code - int - primary key, three-digit code of visitor's origin country\n",
    " * name - varchar - name of the origin country\n",
    " \n",
    "***dim_travel_modes*** - extracted from mapping text, which is provided by Udacity\n",
    " * code - int - primary key, one-digit code of transportation mode\n",
    " * name - varchar - name of the transportation mode\n",
    " \n",
    "***dim_us_visas*** - extracted from mapping text, which is provided by Udacity\n",
    " * code - int - primary key, one-digit code of visa type\n",
    " * name - varchar - name of the visa type\n",
    " \n",
    "***dim_us_date*** - extracted from fact table \n",
    " * arrdate - date - primary key, date of arrival\n",
    " * day - int - day of arrival\n",
    " * weekday - varchar - weekday of arrival\n",
    " * week - int - week of arrival\n",
    " * month - int - month of arrival\n",
    " * year - int - year of arrival\n",
    " \n",
    "***dim_us_ports*** - extracted from mapping text, which is provided by Udacity\n",
    " * code - varchar - primary key, three-character code of destination port/city\n",
    " * city - varchar - city of the arrival\n",
    " * state - varchar - state code of the arrival\n",
    " * state_name - varchar - name of the state of the arrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    - In this project, to build the data model, ETL pipeline and data lake, I have used pandas library, Apache Spark and Amazon S3. I have used pandas for dealing with small datasets like reading text files with mapping data. For processing of large dataset like I94 immigration data (over three million records per month), I have used Apache Spark, because it handles large amounts of data coming from multiple different datasources and data formats with ease. And finally, to implement a data lake, I have used Amazon S3 to store fact and dimension tables in parquet format.\n",
    "* Propose how often the data should be updated and why.\n",
    "    - It depends how often the data is being provided by the transactional system or how often do the end users need the fresh data. My proposal will be to update data on monthly or weekly basis.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "    - In that case, a Spark cluster setup on AWS EMR with be a solution.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - For this purpose, I would introduce a scheduler tool to manage certain tasks. Apache Airflow, with his DAGs, would be a very good solution. \n",
    " * The database needed to be accessed by 100+ people.\n",
    "    - For this purpose, I would propose to load data to Amazon Redshift since it is column-oriented storage, best suited for OLAP workloads and summing over log history, with differnt possible table design strategies. Another solution can be Azure SQl, Oracle Exadata or Terradata Aster.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
